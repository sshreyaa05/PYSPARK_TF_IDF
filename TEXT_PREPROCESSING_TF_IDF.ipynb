{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJfzr2-Ogf2k",
        "outputId": "93cc4d0c-25ac-4174-8c1d-dfd801a30c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## IMPORT NECESSARY LIBRARIES\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.sql.functions import col\n"
      ],
      "metadata": {
        "id": "AR6VlOeSg0pn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CREATING THE SPARK SESSION\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"TextPreprocessingTFIDF\").getOrCreate()"
      ],
      "metadata": {
        "id": "sh04Bdb-g0kU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CREATING THE TEXT DATA MANUALLY\n",
        "data = [\n",
        "    (0, \"I love India\"),\n",
        "    (1, \"India is the best country\"),\n",
        "    (2, \"PySpark makes it easy to work with data\"),\n",
        "    (3, \"I enjoy learning new technologies like Spark\"),\n",
        "    (4, \"Python and Spark are great for data science\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "QlqKwQpCg0iB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DEFINING THE COLUMNS\n",
        "columns = [\"id\", \"text\"]"
      ],
      "metadata": {
        "id": "fHxa_pWsg0fp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CREATING SPARK DATAFRAME\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp9sTP61g0dV",
        "outputId": "3b083bdc-ea1b-4ca6-db8b-99004e0f62ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|                text|\n",
            "+---+--------------------+\n",
            "|  0|        I love India|\n",
            "|  1|India is the best...|\n",
            "|  2|PySpark makes it ...|\n",
            "|  3|I enjoy learning ...|\n",
            "|  4|Python and Spark ...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TOKENIZING THE TEXTS INTO LISTS OF WORDS\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "df_tokens = tokenizer.transform(df)\n",
        "\n",
        "\n",
        "df_tokens.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uka_oM-Lg0a0",
        "outputId": "0290f148-0eb2-41e8-9da9-926e97e0e957"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+\n",
            "| id|                text|               words|\n",
            "+---+--------------------+--------------------+\n",
            "|  0|        I love India|    [i, love, india]|\n",
            "|  1|India is the best...|[india, is, the, ...|\n",
            "|  2|PySpark makes it ...|[pyspark, makes, ...|\n",
            "|  3|I enjoy learning ...|[i, enjoy, learni...|\n",
            "|  4|Python and Spark ...|[python, and, spa...|\n",
            "+---+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## REMOVING THE STOPWORDS FROM TOKENIZED WORDS\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "df_filtered = remover.transform(df_tokens)\n",
        "\n",
        "\n",
        "df_filtered.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kviRuvZqg0YZ",
        "outputId": "1b536656-e933-47a7-b7a0-28c8de2658d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+--------------------+\n",
            "| id|                text|               words|      filtered_words|\n",
            "+---+--------------------+--------------------+--------------------+\n",
            "|  0|        I love India|    [i, love, india]|       [love, india]|\n",
            "|  1|India is the best...|[india, is, the, ...|[india, best, cou...|\n",
            "|  2|PySpark makes it ...|[pyspark, makes, ...|[pyspark, makes, ...|\n",
            "|  3|I enjoy learning ...|[i, enjoy, learni...|[enjoy, learning,...|\n",
            "|  4|Python and Spark ...|[python, and, spa...|[python, spark, g...|\n",
            "+---+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## CONVERTING THE FILTERED WORDS TO TERM FREQUENCY (TF) USING HashingTF\n",
        "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20)\n",
        "df_tfidf = hashing_tf.transform(df_filtered)\n",
        "\n",
        "\n",
        "df_tfidf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYhsmXngg0V3",
        "outputId": "c28b45eb-0150-404e-8735-fc55e47c29a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+--------------------+--------------------+\n",
            "| id|                text|               words|      filtered_words|        raw_features|\n",
            "+---+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0|        I love India|    [i, love, india]|       [love, india]|(20,[0,1],[1.0,1.0])|\n",
            "|  1|India is the best...|[india, is, the, ...|[india, best, cou...|(20,[1,3,10],[1.0...|\n",
            "|  2|PySpark makes it ...|[pyspark, makes, ...|[pyspark, makes, ...|(20,[0,1,7,13,15]...|\n",
            "|  3|I enjoy learning ...|[i, enjoy, learni...|[enjoy, learning,...|(20,[3,5,6,10,12]...|\n",
            "|  4|Python and Spark ...|[python, and, spa...|[python, spark, g...|(20,[3,6,9,10,15]...|\n",
            "+---+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## APPLYING INVERSE DOCUMENT FREQUENCY (IDF) TO GET THE TF-IDF VALUES\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "idf_model = idf.fit(df_tfidf)\n",
        "df_tfidf_final = idf_model.transform(df_tfidf)\n",
        "\n",
        "\n",
        "df_tfidf_final.select(\"id\", \"text\", \"features\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uc9SviQg0TN",
        "outputId": "35670d67-e298-4398-94f3-9f3b73a2be20"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+\n",
            "| id|                text|            features|\n",
            "+---+--------------------+--------------------+\n",
            "|  0|        I love India|(20,[0,1],[0.6931...|\n",
            "|  1|India is the best...|(20,[1,3,10],[0.4...|\n",
            "|  2|PySpark makes it ...|(20,[0,1,7,13,15]...|\n",
            "|  3|I enjoy learning ...|(20,[3,5,6,10,12]...|\n",
            "|  4|Python and Spark ...|(20,[3,6,9,10,15]...|\n",
            "+---+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## STOPPING THE SPARK SESSION\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "fuh5z1TOg0OK"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}